```
% python -m ulmfit lm --dataset-path data/wiki/${LANG}-100 --tokenizer='sp' --nl 4 --name
'nl4' --max-vocab 25000 --lang ${LANG} --qrnn=True - train 10 --bs=50 --drop_mult=0  --label-smoothing-eps=0.10 --tokenizer='sp
Max vocab: 25000
Cache dir: data/wiki/ru-100/models/sp25k
Model dir: data/wiki/ru-100/models/sp25k/qrnn_nl4.m
Wiki text was split to 193047 articles
Wiki text was split to 460 articles
Data lm, trn: 193047, val: 460
Size of vocabulary: 25000
First 20 words in vocab: ['xxunk', 'xxpad', 'xxbos', 'xxfld', 'xxmaj', 'xxup', 'xxrep', 'xxwrep', '<unk>', '▁', '▁,', '▁.', '▁в', 'а', 'и', 'е', '▁и', 'й', 'х', '▁на']
Training args:  {'clip': 0.12, 'alpha': 2, 'beta': 1, 'drop_mult': 0} dps:  {'output_p': 0.25, 'hidden_p': 0.1, 'input_p': 0.2, 'embed_p': 0.02, 'weight_p': 0.15}
Training lm from random weights
epoch     train_loss  valid_loss  accuracy
1         4.154972    4.198218    0.447508
2         4.030367    4.159642    0.449420
3         4.138530    4.146010    0.451526
4         3.997120    4.097048    0.457177
5         3.999151    4.036350    0.465117
6         3.935380    3.955517    0.476446
7         3.912357    3.875987    0.487591
8         3.785693    3.789099    0.501560
9         3.743162    3.725730    0.512294
10        3.690226    3.706929    0.516769
Total time: 12:10:03
data/wiki/ru-100/models/sp25k
Saving info data/wiki/ru-100/models/sp25k/qrnn_nl4.m/info.json
```